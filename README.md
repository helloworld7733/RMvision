# RM自动瞄准系统
---
## 功能概述
本项目基于**OpenCV**实现装甲板识别+数字识别功能，主要模块分为**灯条识别、装甲板识别、装甲板数字识别、pnp位姿估计。**基于三个测试视频，本项目均能较好地识别出视频中的灯条与装甲板，并且具有一定的干扰排除能力，对于激烈的场景下，项目中的跨帧一致性约束功能能很好捕捉位置快速变换的装甲板，具有一定的鲁棒性，经过不断测试优化，系统在运行中不会出现未知报错。

---

## 亮点预览
相较于自动瞄准系统项目的基本要求，本项目的**额外亮点**如下：
1. 设计了PNP位姿估计，能识别出小车距离摄像头的距离（对应加分项第一项）
2. 设计了跨帧一致性约束，能很好地追踪装甲板的运动（对应加分项第二项）
3. 能识别大/小装甲（对应加分项第四项）
4. 设计了误测/重复测装甲板清除算法，减少错误装甲板的干扰

---

## 效果展示与算法讲解
### 灯条识别
在灯条识别中，我首先将`bgr`通道转为更为合适的`hsv`通道，此处通过调整`h`来实现红/蓝颜色切换，通过调整`s`到略微高的值以滤过环境白光，反射光等，通过调整`v`到合适的值滤过环境的暗光和无用信息。之后再进行平滑，膨胀，寻找轮廓等操作。最后我使用外接矩形来拟合边界框，在屏幕上显示出识别出的灯条框。
![lightbar](./readmephoto/lightbar.png "lightbar")

### 装甲板识别
项目依据左右灯条来构造装甲板，并进行筛选，筛选标准有**近似平行，长度相似，中心点y近似，宽长比合适**，同时，在此处依据装甲板的宽长比区分**大装甲和小装甲**。待识别后，考虑到不同装甲板可能出现**重叠现象**，即不同装甲板共享同一个灯条，这在实际中是不可能出现的，于是项目设计了**去重算法**，当识别到不同装甲板共享灯条时，我依据宽长比、倾斜角进行更优筛选，留下最优的匹配。
![armor](./readmephoto/armor.png "armor")

### 装甲板数字识别
我采用**SVM**方法进行数字识别，此处我直接采用导入svm参数的方式，但考虑到视频实时性的问题，项目的svm参数量较小以保证视频流畅，故在识别精度上并不是非常准确，这是我承认的，之后也希望能试图通过其他识别方法或采取更优的训练策略以保证视频实时性与识别精度的平衡。我将数字显示在装甲板左下角处。
![num](./readmephoto/num.png "num")

### 跨帧一致性约束
考虑到小车在实际赛场中运动较为激烈，有时仅仅通过左右灯条对装甲板进行识别可能出现**精度不足**的问题（主要是由于灯条可能出现**视角歪曲，遮挡**等问题，容易被当成错误匹配而舍弃），故项目引入**基于klt光流法的跨帧一致性约束**，这里有趣的是，klt光流法并未在`Opencv入门文档`里提及, 但klt光流法相比于其他特征匹配方法（如BF暴力匹配，FLANN匹配）的优势在于**实时性较好**，故最终我采用此方法进行跨帧一致性约束来辅助匹配。
![klt](./readmephoto/klt.png "klt consistency")
仔细看如上图片我发现，此时画面中的灯条由于较为模糊并未被识别出，但是由于klt光流法成功追踪了前一帧armor角点的运动，故直接在此帧中将追踪出的armor显示出来，也就达成了**不依赖于灯条的装甲板识别**。
此处还有的细节在于，为了进一步提升实时性效果于提高准确率，我对于光流法追踪出的armor直接采用了追踪对象的数字识别结果，而不是重新进行检测。
此外的此外，经过实验发现，有时klt光流法会出现**错误追踪**的情况，导致了装甲板的误识别，于是我在klt光流追踪的最后加入了**错误装甲板识别算法**，去除长宽比不合适、左右灯条上侧角点y值相差过大（说明装甲板**扭曲**）的误识别装甲板，进一步提高了识别精度。

### PNP位姿估计
基于`camera_info.yaml`文件里的相机参数，我使用OpenCV的SolvePnp，传入armor四个角点位置，取得**旋转向量和平移向量**，但考虑到直接显示向量内容没太大意义，于是我转换成了实际信息：对于平移向量，对3个值取平方再开根号能得到**装甲板离摄像头的物理距离**，对不同的值做角度测算可以得到方位角信息；对于旋转向量，我们可以计算**欧拉角**（包含偏航Yaw、俯仰Pitch、滚转Roll），在画面中我仅展示最实用的物理距离，信息显示在装甲板右下角。
![pnp](./readmephoto/pnp.png "pnp位姿估计")

---

## 项目结构
* General: 包含svm参数文件与相机参数文件
    * svm.xml
    * camera_info.yaml
* Video: 用于测试的三段赛场视频
    * test01.mp4
    * test02.mp4
    * test03.mp4
* include: 我的项目的头文件
    * config_manager.h: 包含一些可调参数的全局参数头文件
    * lightbar_detector.h: 灯条识别头文件
    * armor_matcher.h: 装甲板识别头文件
    * num_recognizer.h: 装甲板数字识别头文件
    * perspective.h: pnp位姿估计头文件
* src: 我的项目的源代码
    * main.cpp
    * lightbar_detector.cpp
    * armor_matcher.cpp
    * num_recognizer.cpp
    * perspective.cpp
* CMakeLists.txt: CMake构建文件

---

## 环境依赖
|软件类型|版本|
|---|---|
|Library|OpenCV-4.x|
|Compiler|Visual Studio|
|Operating System|Windows|

---

## 项目构建方式
如果是用vcpkg安装的opencv包，则如下构建：

    mkdir build
    cd build
    cmake -DCMAKE_TOOLCHAIN_FILE=[你的vcpkg路径]/scripts/buildsystems/vcpkg.cmake ..
    cmake --build .

若不是使用vcpkg安装的包，则需要在cmake中设置好OpenCV路径，之后再构建：

    mkdir build
    cd build
    cmake ..
    cmake --build .

使用方式：

    cd build/Debug

打开里面的`Detection.exe`即可

---

## 项目总结与体会
这个项目算是我入门机器人视觉的一个起始点，项目做了整整一周多（因为考试周没时间所以堆到了后面），每天睁眼就是做，一直做到睡前，我也必须承认如果没有网上那么多的开源资料，我无法达到这种进度，这些开源资料是令人尊崇的（包括东南大学，东北大学，吉林大学等），当然我的项目中的每行代码都是我自己慢慢敲出来的，不存在“拿来主义”不假思索直接使用一说，毕竟整个学习的过程是不可替代也不可加速的。我也知道我的项目仍然出现非常多的不足之处，例如svm识别精度不足，混乱场景下装甲板识别不准等问题，我也初步想了一些解决方案，或许我可以把负样本（不含数字的样本，即错误识别的区域图）加入数字识别模型的训练之中？这样我们便可以通过数字识别的结果来进一步判断装甲板区域是否正确识别，或许能进一步提高识别准确率，当然训练的模型可以结合深度学习等技术，不过还是需要考虑实时性（其实我之前训练了一个精度有98但是太太太大的模型导致一个视频放起来要一年）。我非常希望我能加入学校的RM战队算法组，从之前的培训过程和大几十页的培训资料便可以看出这的氛围是极其好的，我也很愿意投入时间钻研比赛，希望我们能双向奔赴！